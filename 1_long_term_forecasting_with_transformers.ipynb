{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2Ldgs8hnZsB9bERSG6ij7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khauzenberger/pytorch-projects/blob/main/1_long_term_forecasting_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The project\n",
        "\n",
        "In a frist step, I'm going to be replicating the paper \"A Time Series is Worth 64 Words: Long-term Forecasting with Transformers\" by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong and Jayant Kalagnanam (see https://arxiv.org/abs/2211.14730).\n",
        "\n",
        "In the second step, then, I apply the key designs of the paper to forecast macroeconomic time series."
      ],
      "metadata": {
        "id": "9nlO-N-HO_nL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key designs, model overview, and key concept\n",
        "\n",
        "**Patching**: Segmentation of time series into subseries-level patches which are served as input tokens to Transformer.\n",
        "\n",
        "**Channel-independence**: Each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series.\n",
        "\n",
        "The figure below sketches the so-called **PatchTST model** and transformer backbones under supervised and self-supervised learning.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/yuqinie98/PatchTST/main/pic/model.png)\n",
        "\n",
        "**Transformer-based models**: It is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data.\n",
        "\n",
        "Transformers are designed to process sequential input data, processing them all at once. The attention mechanism provides context for any position in the input sequence. \n",
        "\n",
        "Because transformers process the entire input all at once, they allow for more parallelization than recurrent neural networks (RNNs) and therefore reduce training times."
      ],
      "metadata": {
        "id": "4yl7wFdbnSbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replication\n",
        "\n",
        "In the following I provide step-by-step instructions to get from our inputs to the desired outputs.\n",
        "\n",
        "While the paper comes with an official implementation (https://github.com/yuqinie98/PatchTST), there's no learning in just copying and pasting them. That's why I will start more or less from scratch, going through the steps in the section on paper replication in Daniel Bourke's ZTM course \"PyTorch for deep learning\" (https://github.com/khauzenberger/pytorch-deep-learning). "
      ],
      "metadata": {
        "id": "6-7Vu2g5pNrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data from the paper\n",
        "\n",
        "While the paper experiments with eight different datasets, I will focus only on the smallest one: the influenza-like illnesses (ILI) dataset. "
      ],
      "metadata": {
        "id": "VUoAGik7E4y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it... \n",
        "if data_path.is_dir():\n",
        "    print(f\"{data_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {data_path} directory, creating one...\")\n",
        "    data_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Download influenza-like illness data from my Github repo\n",
        "    with open(data_path / \"national_illness.csv\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/khauzenberger/pytorch-projects/raw/main/data/national_illness.csv\")      \n",
        "        print(\"Downloading influenza-like illness data from my Github repo...\")\n",
        "        f.write(request.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_9Ic_aqJOkn",
        "outputId": "4d9868bb-cf9c-4541-ff84-1b3dfb656844"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find data directory, creating one...\n",
            "Downloading influenza-like illness data from my Github repo...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Datasets and DataLoaders"
      ],
      "metadata": {
        "id": "ZxyQudfCvdCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def create_dataloaders(data_path:str, file_name:str, seq_len:int, pred_len:int, \n",
        "                       batch_size:int=1, scale=False,features:str=\"S\",\n",
        "                       target:str=\"OT\"):\n",
        "  \n",
        "  scaler = StandardScaler()\n",
        "  df_raw = pd.read_csv(os.path.join(data_path,file_name))\n",
        "  \n",
        "  cols = list(df_raw.columns)\n",
        "  cols.remove(target)\n",
        "  cols.remove(\"date\")\n",
        "  df_raw = df_raw[[\"date\"] + cols + [target]]\n",
        "\n",
        "  num_train = int(len(df_raw) * 0.7)\n",
        "  num_test = int(len(df_raw) * 0.2)\n",
        "  num_vali = len(df_raw) - num_train - num_test\n",
        "\n",
        "  border1s = [0, num_train - seq_len, len(df_raw) - num_vali - seq_len]\n",
        "  border2s = [num_train, num_train + num_test, len(df_raw)]\n",
        "\n",
        "  if features == \"M\" or features == \"MS\":\n",
        "    cols_data = df_raw.columns[1:]\n",
        "    df_data = df_raw[cols_data]\n",
        "  elif features == \"S\":\n",
        "    df_data = df_raw[[target]]\n",
        "\n",
        "  if scale:\n",
        "    train_data = df_data[border1s[0]:border2s[0]]\n",
        "    scaler.fit(train_data.values)\n",
        "    data = scaler.transform(df_data.values)\n",
        "  else:\n",
        "    data = df_data.values\n",
        "\n",
        "  train_data = data[border1s[0]:border2s[0]]\n",
        "  test_data = data[border1s[1]:border2s[1]]\n",
        "  vali_data = data[border1s[2]:border2s[2]]\n",
        "\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True)\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False)\n",
        "  vali_dataloader = DataLoader(\n",
        "      vali_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False)\n",
        "\n",
        "  return train_dataloader, test_dataloader, vali_dataloader"
      ],
      "metadata": {
        "id": "NZxkLb0MYXMh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model settings"
      ],
      "metadata": {
        "id": "bRnZHGkv0OjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup path to data folder and file name\n",
        "data_path = Path(\"data/\")\n",
        "file_name = \"national_illness.csv\"\n",
        "\n",
        "# Sequence length (look-back window)\n",
        "seq_len = 104\n",
        "\n",
        "# Prediciton length (forecast horizon)\n",
        "pred_len = 24\n",
        "\n",
        "# Batch size\n",
        "batch_size = 16\n",
        "\n",
        "# Features of model: M  -> multivariate predict multivariate\n",
        "#                    S  -> univariate predict univariate\n",
        "#                    MS -> multivariate predict univariate\n",
        "features = \"M\"\n",
        "\n",
        "# Various parameters and hyperparameters\n",
        "d_model = 16\n",
        "patch_len = 24\n",
        "stride = 2\n",
        "n_heads = 4\n",
        "enc_in = 7\n",
        "e_layers = 3\n",
        "d_ff = 128\n",
        "dropout = 0.3\n",
        "fc_dropout = 0.3\n",
        "head_dropout = 0\n",
        "learning_rate = 0.0025\n",
        "padding_patch=\"end\""
      ],
      "metadata": {
        "id": "dPnqG489Jfm3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PatchTST architecture\n",
        "\n",
        "### Instance normalization\n",
        "\n",
        "This technique helps mitigating the distribution shift effect between the training and testing data."
      ],
      "metadata": {
        "id": "BBEgrUA_M9HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class InstanceNorm(nn.Module):\n",
        "  def __init__(self, num_features:int, eps=1e-5,  affine=True, \n",
        "               subtract_last=False):    \n",
        "    \"\"\"\n",
        "    :param num_features: the number of features or channels\n",
        "    :param eps: a value added for numerical stability\n",
        "    :param affine: if True, InstanceNorm has learnable affine parameters\n",
        "    \"\"\"    \n",
        "    super(InstanceNorm, self).__init__()\n",
        "    self.num_features = num_features\n",
        "    self.eps = eps\n",
        "    self.affine = affine\n",
        "    self.subtract_last = subtract_last\n",
        "    if self.affine:\n",
        "      self._init_params()\n",
        "\n",
        "  def forward(self, x, mode:str):\n",
        "    if mode == \"norm\":\n",
        "      self._get_statistics(x)\n",
        "      x = self._normalize(x)\n",
        "    elif mode == \"denorm\":\n",
        "      x = self._denormalize(x)\n",
        "    else: raise NotImplementedError\n",
        "    return x\n",
        "\n",
        "  def _init_params(self):\n",
        "    # initialize InstanceNorm params: (C,)\n",
        "    self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
        "    self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
        "\n",
        "  def _get_statistics(self, x):\n",
        "    dim2reduce = tuple(range(1, x.ndim-1))\n",
        "    if self.subtract_last:\n",
        "      self.last = x[:,-1,:].unsqueeze(1)\n",
        "    else:\n",
        "      self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
        "      self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
        "\n",
        "  def _normalize(self, x):\n",
        "    if self.subtract_last:\n",
        "      x = x - self.last\n",
        "    else:\n",
        "      x = x - self.mean\n",
        "      x = x / self.stdev\n",
        "    if self.affine:\n",
        "      x = x * self.affine_weight\n",
        "      x = x + self.affine_bias\n",
        "    return x\n",
        "\n",
        "  def _denormalize(self, x):\n",
        "    if self.affine:\n",
        "      x = x - self.affine_bias\n",
        "      x = x / (self.affine_weight + self.eps*self.eps)\n",
        "      x = x * self.stdev\n",
        "    if self.subtract_last:\n",
        "      x = x + self.last\n",
        "    else:\n",
        "      x = x + self.mean\n",
        "    return x"
      ],
      "metadata": {
        "id": "TsvSnKiQNUpN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_data = torch.randn(10,7)\n",
        "instnorm = InstanceNorm(num_features=7)\n",
        "instnorm(random_data,\"norm\")"
      ],
      "metadata": {
        "id": "wGU0UJR4TttY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Patching and Padding\n",
        "\n",
        "Pads the input tensor using replication of the input boundary.\n",
        "\n"
      ],
      "metadata": {
        "id": "AW4NJxzOzvEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.arraypad import pad\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "patch_num = int((seq_len-patch_len)/stride+1)\n",
        "if padding_patch == \"end\":\n",
        "  padding_patch_layer = nn.ReplicationPad1d((0, stride))\n",
        "  patch_num += 1"
      ],
      "metadata": {
        "id": "MU_-nBMAzzzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input encoding (projection)\n",
        "\n",
        "Implemeting equation 1: Projection of feature vectors onto a d-dim vector space."
      ],
      "metadata": {
        "id": "ZfJXKRke1-oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_P = nn.Linear(patch_len, d_model)"
      ],
      "metadata": {
        "id": "Ytk6rxrX2ETs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional encoding (position embedding)"
      ],
      "metadata": {
        "id": "w_SnLQ523NOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from typing import Callable, Optional\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def PositionalEncoding(pe, learn_pe, q_len, d_model):\n",
        "    if pe == None:\n",
        "      W_pos = torch.empty((q_len, d_model))\n",
        "      nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "      learn_pe = False\n",
        "    elif pe == \"zero\":\n",
        "        W_pos = torch.empty((q_len, 1))\n",
        "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "    elif pe == \"zeros\":\n",
        "        W_pos = torch.empty((q_len, d_model))\n",
        "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "    elif pe == \"normal\" or pe == \"gauss\":\n",
        "        W_pos = torch.zeros((q_len, 1))\n",
        "        torch.nn.init.normal_(W_pos, mean=0.0, std=0.1)\n",
        "    elif pe == \"uniform\":\n",
        "        W_pos = torch.zeros((q_len, 1))\n",
        "        nn.init.uniform_(W_pos, a=0.0, b=0.1)\n",
        "    else: raise ValueError(f\"{pe} is not a valid pe (positional encoder). Available types: 'gauss'=='normal', \\\n",
        "        'zeros', 'zero', 'uniform'.\")\n",
        "    return nn.Parameter(W_pos, requires_grad=learn_pe)"
      ],
      "metadata": {
        "id": "a1QsS33J3r-h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backbone"
      ],
      "metadata": {
        "id": "dRxRzoVH5tuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, Optional\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class TransformerBackbone(nn.Module):\n",
        "  def __init__(self, c_in:int, context_window:int, target_window:int, patch_len:int,\n",
        "               stride:int, max_seq_len:Optional[int]=1024, n_layers:int=3, d_model=128,\n",
        "               n_heads=16, d_k:Optional[int]=None, d_v:Optional[int]=None, d_ff:int=256, \n",
        "               norm:str='BatchNorm', attn_dropout:float=0., dropout:float=0., act:str=\"gelu\", \n",
        "               key_padding_mask:bool='auto', padding_var:Optional[int]=None, \n",
        "               attn_mask:Optional[Tensor]=None, res_attention:bool=True, pre_norm:bool=False, \n",
        "               store_attn:bool=False, pe:str='zeros', learn_pe:bool=True, fc_dropout:float=0., \n",
        "               head_dropout = 0, padding_patch = None, pretrain_head:bool=False, \n",
        "               head_type = 'flatten', individual = False, insnorm = True, affine = True, \n",
        "               subtract_last = False, verbose:bool=False, **kwargs):\n",
        "        \n",
        "    super().__init__()\n",
        "\n",
        "    # Instance normalization\n",
        "    self.insnorm = insnorm\n",
        "    if insnorm:\n",
        "      self.insnorm_layer = InstanceNorm(c_in, eps=1e-5, affine=affine,\n",
        "                                        subtract_last=subtract_last)\n",
        "    \n",
        "    # Patching and padding\n",
        "    self.patch_len = patch_len\n",
        "    self.stride = stride\n",
        "    self.padding_patch = padding_patch\n",
        "    patch_num = int((seq_len-patch_len)/stride+1)\n",
        "    if padding_patch == \"end\":\n",
        "      self.padding_patch_layer = nn.ReplicationPad1d((0, stride))\n",
        "      patch_num += 1\n",
        "\n",
        "    # Backbone \n",
        "    self.backbone = TransformerIndependentEncoder(c_in, patch_num=patch_num, patch_len=patch_len, \n",
        "                                                  max_seq_len=max_seq_len, n_layers=n_layers, d_model=d_model, \n",
        "                                                  n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n",
        "                                                  attn_dropout=attn_dropout, dropout=dropout, act=act, \n",
        "                                                  key_padding_mask=key_padding_mask, padding_var=padding_var,\n",
        "                                                  attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, \n",
        "                                                  store_attn=store_attn, pe=pe, learn_pe=learn_pe, verbose=verbose, \n",
        "                                                  **kwargs)\n",
        "    \n",
        "    # Head\n",
        "    self.head_nf = d_model * patch_num\n",
        "    self.n_vars = c_in\n",
        "    self.pretrain_head = pretrain_head\n",
        "    self.head_type = head_type\n",
        "    self.individual = individual    \n",
        "\n",
        "    if self.pretrain_head: \n",
        "      # custom head passed as a partial func with all its kwargs\n",
        "      self.head = self.create_pretrain_head(self.head_nf, c_in, fc_dropout) \n",
        "    elif head_type == 'flatten': \n",
        "      self.head = FlattenHead(self.individual, self.n_vars, self.head_nf, \n",
        "                              target_window, head_dropout=head_dropout)\n",
        "\n",
        "    def create_pretrain_head(self, head_nf, vars, dropout):\n",
        "      return nn.Sequential(nn.Dropout(dropout), nn.Conv1d(head_nf, vars, 1))\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, z):                            \n",
        "      # Instance normalization\n",
        "      if self.insnorm:\n",
        "        z = z.permute(0,2,1)                                             # z: [bs x nvars x seq_len]\n",
        "        z = self.revin_layer(z, \"norm\")\n",
        "        z = z.permute(0,2,1)\n",
        "            \n",
        "      # Patching\n",
        "      if self.padding_patch == \"end\":\n",
        "        z = self.padding_patch_layer(z)\n",
        "      z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)  # z: [bs x nvars x patch_num x patch_len]           \n",
        "      z = z.permute(0,1,3,2)                                             # z: [bs x nvars x patch_len x patch_num]                                                             \n",
        "        \n",
        "      # Model\n",
        "      z = self.backbone(z)                                               # z: [bs x nvars x d_model x patch_num]\n",
        "      z = self.head(z)                                                   # z: [bs x nvars x target_window] \n",
        "        \n",
        "      # Instance denormalization\n",
        "      if self.insnorm: \n",
        "        z = z.permute(0,2,1)\n",
        "        z = self.revin_layer(z, \"denorm\")\n",
        "        z = z.permute(0,2,1)\n",
        "      return z      "
      ],
      "metadata": {
        "id": "BQQVI47x553F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "\n",
        "class TransformerIndependentEncoder(nn.Module):\n",
        "  def __init__(self, c_in, patch_num, patch_len, max_seq_len=1024, n_layers=3, \n",
        "               d_model=128, n_heads=16, d_k=None, d_v=None,d_ff=256, norm='BatchNorm', \n",
        "               attn_dropout=0., dropout=0., act=\"gelu\", store_attn=False,\n",
        "               key_padding_mask='auto', padding_var=None, attn_mask=None, \n",
        "               res_attention=True, pre_norm=False, pe='zeros', learn_pe=True, \n",
        "               verbose=False, **kwargs):\n",
        "               \n",
        "    super().__init__()    \n",
        "    \n",
        "    # Input encoding\n",
        "    self.W_P = nn.Linear(patch_len, d_model)\n",
        "\n",
        "    # Positional encoding\n",
        "    self.W_pos = PositionalEncoding(pe, learn_pe, patch_num, d_model)\n",
        "\n",
        "    # Residual dropout\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Encoder\n",
        "    self.encoder = TransformerEncoder(patch_num, d_model, n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n",
        "                                      norm=norm, attn_dropout=attn_dropout, dropout=dropout,\n",
        "                                      pre_norm=pre_norm, activation=act, res_attention=res_attention, \n",
        "                                      n_layers=n_layers, store_attn=store_attn)\n",
        "   \n",
        "    def forward(self, x) -> Tensor:                                            # x: [bs x nvars x patch_len x patch_num]\n",
        "        \n",
        "      # Input encoding\n",
        "      x = x.permute(0,1,3,2)                                                   # x: [bs x nvars x patch_num x patch_len]\n",
        "      x = self.W_P(x)                                                          # x: [bs x nvars x patch_num x d_model]\n",
        "\n",
        "      u = torch.reshape(x, (x.shape[0]*x.shape[1],x.shape[2],x.shape[3]))      # u: [bs * nvars x patch_num x d_model]\n",
        "      u = self.dropout(u + self.W_pos)                                         # u: [bs * nvars x patch_num x d_model]\n",
        "\n",
        "      # Encoder\n",
        "      z = self.encoder(u)                                                      # z: [bs * nvars x patch_num x d_model]\n",
        "      z = torch.reshape(z, (-1,x.shape[1],z.shape[-2],z.shape[-1]))            # z: [bs x nvars x patch_num x d_model]\n",
        "      z = z.permute(0,1,3,2)                                                   # z: [bs x nvars x d_model x patch_num]\n",
        "        \n",
        "      return z   "
      ],
      "metadata": {
        "id": "a_OoxV89UgNW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FlattenHead(nn.Module):\n",
        "  def __init__(self, individual, n_vars, nf, target_window, head_dropout=0):\n",
        "    super().__init__()\n",
        "        \n",
        "    self.individual = individual\n",
        "    self.n_vars = n_vars\n",
        "        \n",
        "    if self.individual:\n",
        "      self.linears = nn.ModuleList()\n",
        "      self.dropouts = nn.ModuleList()\n",
        "      self.flattens = nn.ModuleList()\n",
        "      for i in range(self.n_vars):\n",
        "        self.flattens.append(nn.Flatten(start_dim=-2))\n",
        "        self.linears.append(nn.Linear(nf, target_window))\n",
        "        self.dropouts.append(nn.Dropout(head_dropout))\n",
        "      else:\n",
        "        self.flatten = nn.Flatten(start_dim=-2)\n",
        "        self.linear = nn.Linear(nf, target_window)\n",
        "        self.dropout = nn.Dropout(head_dropout)\n",
        "            \n",
        "  def forward(self, x):                     # x: [bs x nvars x d_model x patch_num]\n",
        "    if self.individual:\n",
        "      x_out = []\n",
        "      for i in range(self.n_vars):\n",
        "        z = self.flattens[i](x[:,i,:,:])    # z: [bs x d_model * patch_num]\n",
        "        z = self.linears[i](z)              # z: [bs x target_window]\n",
        "        z = self.dropouts[i](z)\n",
        "        x_out.append(z)\n",
        "        x = torch.stack(x_out, dim=1)       # x: [bs x nvars x target_window]\n",
        "      else:\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.dropout(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "TedS_CgeO_ih"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, Optional\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=None, norm='BatchNorm', \n",
        "               attn_dropout=0., dropout=0., activation='gelu',\n",
        "               res_attention=False, n_layers=1, pre_norm=False, store_attn=False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = nn.ModuleList([TransformerEncoderLayer(q_len, d_model, n_heads=n_heads, d_k=d_k, \n",
        "                                                         d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout, \n",
        "                                                         dropout=dropout, activation=activation, \n",
        "                                                         res_attention=res_attention, pre_norm=pre_norm, \n",
        "                                                         store_attn=store_attn) for i in range(n_layers)])\n",
        "    self.res_attention = res_attention\n",
        "\n",
        "  def forward(self, src:Tensor, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
        "    output = src\n",
        "    scores = None\n",
        "    if self.res_attention:\n",
        "      for mod in self.layers: output, scores = mod(output, prev=scores, key_padding_mask=key_padding_mask, \n",
        "                                                   attn_mask=attn_mask)\n",
        "      return output\n",
        "    else:\n",
        "      for mod in self.layers: output = mod(output, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "      return output\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=256, store_attn=False,\n",
        "               norm='BatchNorm', attn_dropout=0, dropout=0., bias=True, activation=\"gelu\", \n",
        "               res_attention=False, pre_norm=False):\n",
        "    super().__init__()\n",
        "        \n",
        "    assert not d_model%n_heads, f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
        "    d_k = d_model // n_heads if d_k is None else d_k\n",
        "    d_v = d_model // n_heads if d_v is None else d_v\n",
        "\n",
        "    # Multi-Head attention\n",
        "    self.res_attention = res_attention\n",
        "    self.self_attn = _MultiheadAttention(d_model, n_heads, d_k, d_v, attn_dropout=attn_dropout, \n",
        "                                         proj_dropout=dropout, res_attention=res_attention)\n",
        "\n",
        "    # Add & Norm\n",
        "    self.dropout_attn = nn.Dropout(dropout)\n",
        "    if \"batch\" in norm.lower():\n",
        "      self.norm_attn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
        "    else:\n",
        "      self.norm_attn = nn.LayerNorm(d_model)\n",
        "\n",
        "    # Position-wise Feed-Forward\n",
        "    self.ff = nn.Sequential(nn.Linear(d_model, d_ff, bias=bias), get_activation_fn(activation),\n",
        "                            nn.Dropout(dropout), nn.Linear(d_ff, d_model, bias=bias))\n",
        "\n",
        "    # Add & Norm\n",
        "    self.dropout_ffn = nn.Dropout(dropout)\n",
        "    if \"batch\" in norm.lower():\n",
        "      self.norm_ffn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
        "    else:\n",
        "      self.norm_ffn = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.pre_norm = pre_norm\n",
        "    self.store_attn = store_attn\n",
        "\n",
        "\n",
        "  def forward(self, src:Tensor, prev:Optional[Tensor]=None, key_padding_mask:Optional[Tensor]=None, \n",
        "              attn_mask:Optional[Tensor]=None) -> Tensor:\n",
        "\n",
        "    # Multi-Head attention sublayer\n",
        "    if self.pre_norm:\n",
        "      src = self.norm_attn(src)\n",
        "    ## Multi-Head attention\n",
        "    if self.res_attention:\n",
        "      src2, attn, scores = self.self_attn(src, src, src, prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "    else:\n",
        "      src2, attn = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "    if self.store_attn:\n",
        "      self.attn = attn\n",
        "    ## Add & Norm\n",
        "    src = src + self.dropout_attn(src2) # Add: residual connection with residual dropout\n",
        "    if not self.pre_norm:\n",
        "      src = self.norm_attn(src)\n",
        "\n",
        "    # Feed-forward sublayer\n",
        "    if self.pre_norm:\n",
        "      src = self.norm_ffn(src)\n",
        "    ## Position-wise Feed-Forward\n",
        "    src2 = self.ff(src)\n",
        "    ## Add & Norm\n",
        "    src = src + self.dropout_ffn(src2) # Add: residual connection with residual dropout\n",
        "    if not self.pre_norm:\n",
        "      src = self.norm_ffn(src)\n",
        "\n",
        "    if self.res_attention:\n",
        "      return src, scores\n",
        "    else:\n",
        "      return src\n",
        "\n",
        "\n",
        "class _MultiheadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, d_k=None, d_v=None, res_attention=False, attn_dropout=0., \n",
        "               proj_dropout=0., qkv_bias=True, lsa=False):\n",
        "    \"\"\"Multi Head Attention Layer\n",
        "    Input shape:\n",
        "    Q:       [batch_size (bs) x max_q_len x d_model]\n",
        "    K, V:    [batch_size (bs) x q_len x d_model]\n",
        "    mask:    [q_len x q_len]\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    d_k = d_model // n_heads if d_k is None else d_k\n",
        "    d_v = d_model // n_heads if d_v is None else d_v\n",
        "\n",
        "    self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n",
        "\n",
        "    self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
        "    self.W_K = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
        "    self.W_V = nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n",
        "\n",
        "    # Scaled Dot-Product Attention (multiple heads)\n",
        "    self.res_attention = res_attention\n",
        "    self.sdp_attn = _ScaledDotProductAttention(d_model, n_heads, attn_dropout=attn_dropout, \n",
        "                                               res_attention=self.res_attention, lsa=lsa)\n",
        "\n",
        "    # Poject output\n",
        "    self.to_out = nn.Sequential(nn.Linear(n_heads * d_v, d_model), nn.Dropout(proj_dropout))\n",
        "\n",
        "\n",
        "  def forward(self, Q:Tensor, K:Optional[Tensor]=None, V:Optional[Tensor]=None, \n",
        "              prev:Optional[Tensor]=None, key_padding_mask:Optional[Tensor]=None, \n",
        "              attn_mask:Optional[Tensor]=None):\n",
        "\n",
        "    bs = Q.size(0)\n",
        "    if K is None: K = Q\n",
        "    if V is None: V = Q\n",
        "\n",
        "    # Linear (+ split in multiple heads)\n",
        "    q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1,2)       # q_s    : [bs x n_heads x max_q_len x d_k]\n",
        "    k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0,2,3,1)     # k_s    : [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n",
        "    v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1,2)       # v_s    : [bs x n_heads x q_len x d_v]\n",
        "\n",
        "    # Apply Scaled Dot-Product Attention (multiple heads)\n",
        "    if self.res_attention:\n",
        "      output, attn_weights, attn_scores = self.sdp_attn(q_s, k_s, v_s, prev=prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "    else:\n",
        "      output, attn_weights = self.sdp_attn(q_s, k_s, v_s, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
        "    # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n",
        "\n",
        "    # Back to the original inputs dimensions\n",
        "    output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v) \n",
        "    # output: [bs x q_len x n_heads * d_v]\n",
        "    output = self.to_out(output)\n",
        "\n",
        "    if self.res_attention: return output, attn_weights, attn_scores\n",
        "    else: return output, attn_weights\n",
        "\n",
        "\n",
        "class _ScaledDotProductAttention(nn.Module):\n",
        "  r\"\"\"Scaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with optional residual attention from previous layer\n",
        "  (Realformer: Transformer likes residual attention by He et al, 2020) and locality self sttention (Vision Transformer for Small-Size Datasets\n",
        "  by Lee et al, 2021)\"\"\"\n",
        "\n",
        "  def __init__(self, d_model, n_heads, attn_dropout=0., res_attention=False, lsa=False):\n",
        "    super().__init__()\n",
        "    self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "    self.res_attention = res_attention\n",
        "    head_dim = d_model // n_heads\n",
        "    self.scale = nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n",
        "    self.lsa = lsa\n",
        "\n",
        "  def forward(self, q:Tensor, k:Tensor, v:Tensor, prev:Optional[Tensor]=None, \n",
        "              key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
        "    '''\n",
        "    Input shape:\n",
        "      q               : [bs x n_heads x max_q_len x d_k]\n",
        "      k               : [bs x n_heads x d_k x seq_len]\n",
        "      v               : [bs x n_heads x seq_len x d_v]\n",
        "      prev            : [bs x n_heads x q_len x seq_len]\n",
        "      key_padding_mask: [bs x seq_len]\n",
        "      attn_mask       : [1 x seq_len x seq_len]\n",
        "    Output shape:\n",
        "      output:  [bs x n_heads x q_len x d_v]\n",
        "      attn   : [bs x n_heads x q_len x seq_len]\n",
        "      scores : [bs x n_heads x q_len x seq_len]\n",
        "    '''\n",
        "\n",
        "    # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n",
        "    attn_scores = torch.matmul(q, k) * self.scale      # attn_scores : [bs x n_heads x max_q_len x q_len]\n",
        "\n",
        "    # Add pre-softmax attention scores from the previous layer (optional)\n",
        "    if prev is not None: attn_scores = attn_scores + prev\n",
        "\n",
        "    # Attention mask (optional)\n",
        "    if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n",
        "      if attn_mask.dtype == torch.bool:\n",
        "        attn_scores.masked_fill_(attn_mask, -np.inf)\n",
        "      else:\n",
        "        attn_scores += attn_mask\n",
        "\n",
        "    # Key padding mask (optional)\n",
        "    if key_padding_mask is not None:                              # mask with shape [bs x q_len] (only when max_w_len == q_len)\n",
        "      attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf)\n",
        "\n",
        "    # Normalize the attention weights\n",
        "    attn_weights = F.softmax(attn_scores, dim=-1)                 # attn_weights   : [bs x n_heads x max_q_len x q_len]\n",
        "    attn_weights = self.attn_dropout(attn_weights)\n",
        "\n",
        "    # Compute the new values given the attention weights\n",
        "    output = torch.matmul(attn_weights, v)                        # output: [bs x n_heads x max_q_len x d_v]\n",
        "\n",
        "    if self.res_attention: return output, attn_weights, attn_scores\n",
        "    else: return output, attn_weights\n",
        "\n",
        "\n",
        "class Transpose(nn.Module):\n",
        "  def __init__(self, *dims, contiguous=False): \n",
        "    super().__init__()\n",
        "    self.dims, self.contiguous = dims, contiguous\n",
        "  def forward(self, x):\n",
        "    if self.contiguous: return x.transpose(*self.dims).contiguous()\n",
        "    else: return x.transpose(*self.dims)\n",
        "\n",
        "    \n",
        "def get_activation_fn(activation):\n",
        "  if callable(activation): return activation()\n",
        "  elif activation.lower() == \"relu\": return nn.ReLU()\n",
        "  elif activation.lower() == \"gelu\": return nn.GELU()\n",
        "  raise ValueError(f'{activation} is not available. You can use \"relu\", \"gelu\", or a callable') \n",
        "           "
      ],
      "metadata": {
        "id": "-UF5a0pHaWuJ"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}