{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPhRqq6CUD9XyImfAYBn0YH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The project\n",
        "\n",
        "In a frist step, I'm going to be replicating the paper \"A Time Series is Worth 64 Words: Long-term Forecasting with Transformers\" by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong and Jayant Kalagnanam (see https://arxiv.org/abs/2211.14730).\n",
        "\n",
        "In the second step, then, I apply the key designs of the paper to forecast macroeconomic time series."
      ],
      "metadata": {
        "id": "9nlO-N-HO_nL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key designs, model overview, and key concept\n",
        "\n",
        "**Patching**: Segmentation of time series into subseries-level patches which are served as input tokens to Transformer.\n",
        "\n",
        "**Channel-independence**: Each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series.\n",
        "\n",
        "The figure below sketches the so-called **PatchTST model** and transformer backbones under supervised and self-supervised learning.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/yuqinie98/PatchTST/main/pic/model.png)\n",
        "\n",
        "**Transformer-based models**: It is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data.\n",
        "\n",
        "Transformers are designed to process sequential input data, processing them all at once. The attention mechanism provides context for any position in the input sequence. \n",
        "\n",
        "Because transformers process the entire input all at once, they allow for more parallelization than recurrent neural networks (RNNs) and therefore reduce training times."
      ],
      "metadata": {
        "id": "4yl7wFdbnSbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replication\n",
        "\n",
        "In the following I provide step-by-step instructions to get from our inputs to the desired outputs.\n",
        "\n",
        "While the paper comes with an official implementation (https://github.com/yuqinie98/PatchTST), there's no learning in just copying and pasting them. That's why I will start more or less from scratch, going through the steps in the section on paper replication in Daniel Bourke's ZTM course \"PyTorch for deep learning\" (https://github.com/khauzenberger/pytorch-deep-learning). "
      ],
      "metadata": {
        "id": "6-7Vu2g5pNrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data from the paper\n",
        "\n",
        "While the paper experiments with eight different datasets, I will focus only on the smallest one: the influenza-like illnesses (ILI) dataset. "
      ],
      "metadata": {
        "id": "VUoAGik7E4y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it... \n",
        "if data_path.is_dir():\n",
        "    print(f\"{data_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {data_path} directory, creating one...\")\n",
        "    data_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Download influenza-like illness data from my Github repo\n",
        "    with open(data_path / \"national_illness.csv\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/khauzenberger/pytorch-projects/raw/main/data/national_illness.csv\")      \n",
        "        print(\"Downloading influenza-like illness data from my Github repo...\")\n",
        "        f.write(request.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_9Ic_aqJOkn",
        "outputId": "6be71fc4-1c26-4990-9ae4-f7188317dc64"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find data directory, creating one...\n",
            "Downloading influenza-like illness data from my Github repo...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the architecture\n",
        "\n",
        "I start by going through the figure above, and try to explain how each stage relates to econometric-style time series analysis.\n",
        "\n",
        "*   **Patching + Instance Normalization** - \n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "ZxyQudfCvdCq"
      }
    }
  ]
}