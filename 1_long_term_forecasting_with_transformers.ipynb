{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSUVfRHojhlTa2d5LByLsz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khauzenberger/pytorch-projects/blob/main/1_long_term_forecasting_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The project\n",
        "\n",
        "In a frist step, I'm going to be replicating the paper \"A Time Series is Worth 64 Words: Long-term Forecasting with Transformers\" by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong and Jayant Kalagnanam (see https://arxiv.org/abs/2211.14730).\n",
        "\n",
        "In the second step, then, I apply the key designs of the paper to forecast macroeconomic time series."
      ],
      "metadata": {
        "id": "9nlO-N-HO_nL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key designs, model overview, and key concept\n",
        "\n",
        "**Patching**: Segmentation of time series into subseries-level patches which are served as input tokens to Transformer.\n",
        "\n",
        "**Channel-independence**: Each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series.\n",
        "\n",
        "The figure below sketches the so-called **PatchTST model** and transformer backbones under supervised and self-supervised learning.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/yuqinie98/PatchTST/main/pic/model.png)\n",
        "\n",
        "**Transformer-based models**: It is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data.\n",
        "\n",
        "Transformers are designed to process sequential input data, processing them all at once. The attention mechanism provides context for any position in the input sequence. \n",
        "\n",
        "Because transformers process the entire input all at once, they allow for more parallelization than recurrent neural networks (RNNs) and therefore reduce training times."
      ],
      "metadata": {
        "id": "4yl7wFdbnSbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replication\n",
        "\n",
        "In the following I provide step-by-step instructions to get from our inputs to the desired outputs.\n",
        "\n",
        "While the paper comes with an official implementation (https://github.com/yuqinie98/PatchTST), there's no learning in just copying and pasting them. That's why I will start more or less from scratch, going through the steps in the section on paper replication in Daniel Bourke's ZTM course \"PyTorch for deep learning\" (https://github.com/khauzenberger/pytorch-deep-learning). "
      ],
      "metadata": {
        "id": "6-7Vu2g5pNrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data from the paper\n",
        "\n",
        "While the paper experiments with eight different datasets, I will focus only on the smallest one: the influenza-like illnesses (ILI) dataset. "
      ],
      "metadata": {
        "id": "VUoAGik7E4y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it... \n",
        "if data_path.is_dir():\n",
        "    print(f\"{data_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {data_path} directory, creating one...\")\n",
        "    data_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Download influenza-like illness data from my Github repo\n",
        "    with open(data_path / \"national_illness.csv\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/khauzenberger/pytorch-projects/raw/main/data/national_illness.csv\")      \n",
        "        print(\"Downloading influenza-like illness data from my Github repo...\")\n",
        "        f.write(request.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_9Ic_aqJOkn",
        "outputId": "7bf248d4-6f70-410b-f7c0-6875c549f3e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find data directory, creating one...\n",
            "Downloading influenza-like illness data from my Github repo...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Datasets and DataLoaders"
      ],
      "metadata": {
        "id": "ZxyQudfCvdCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def create_dataloaders(data_path:str, file_name:str, seq_len:int, pred_len:int, \n",
        "                       batch_size:int=1, scale=False,features:str=\"S\",\n",
        "                       target:str=\"OT\"):\n",
        "  \n",
        "  scaler = StandardScaler()\n",
        "  df_raw = pd.read_csv(os.path.join(data_path,file_name))\n",
        "  \n",
        "  cols = list(df_raw.columns)\n",
        "  cols.remove(target)\n",
        "  cols.remove(\"date\")\n",
        "  df_raw = df_raw[[\"date\"] + cols + [target]]\n",
        "\n",
        "  num_train = int(len(df_raw) * 0.7)\n",
        "  num_test = int(len(df_raw) * 0.2)\n",
        "  num_vali = len(df_raw) - num_train - num_test\n",
        "\n",
        "  border1s = [0, num_train - seq_len, len(df_raw) - num_vali - seq_len]\n",
        "  border2s = [num_train, num_train + num_test, len(df_raw)]\n",
        "\n",
        "  if features == \"M\" or features == \"MS\":\n",
        "    cols_data = df_raw.columns[1:]\n",
        "    df_data = df_raw[cols_data]\n",
        "  elif features == \"S\":\n",
        "    df_data = df_raw[[target]]\n",
        "\n",
        "  if scale:\n",
        "    train_data = df_data[border1s[0]:border2s[0]]\n",
        "    scaler.fit(train_data.values)\n",
        "    data = scaler.transform(df_data.values)\n",
        "  else:\n",
        "    data = df_data.values\n",
        "\n",
        "  train_data = data[border1s[0]:border2s[0]]\n",
        "  test_data = data[border1s[1]:border2s[1]]\n",
        "  vali_data = data[border1s[2]:border2s[2]]\n",
        "\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True)\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False)\n",
        "  vali_dataloader = DataLoader(\n",
        "      vali_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False)\n",
        "\n",
        "  return train_dataloader, test_dataloader, vali_dataloader"
      ],
      "metadata": {
        "id": "NZxkLb0MYXMh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instance normalization\n",
        "\n",
        "This technique helps mitigating the distribution shift effect between the training and testing data."
      ],
      "metadata": {
        "id": "BBEgrUA_M9HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class InstanceNorm(nn.Module):\n",
        "  def __init__(self, num_features:int, eps=1e-5,  affine=True, \n",
        "               subtract_last=False):    \n",
        "    \"\"\"\n",
        "    :param num_features: the number of features or channels\n",
        "    :param eps: a value added for numerical stability\n",
        "    :param affine: if True, InstanceNorm has learnable affine parameters\n",
        "    \"\"\"    \n",
        "    super(InstanceNorm, self).__init__()\n",
        "    self.num_features = num_features\n",
        "    self.eps = eps\n",
        "    self.affine = affine\n",
        "    self.subtract_last = subtract_last\n",
        "    if self.affine:\n",
        "      self._init_params()\n",
        "\n",
        "  def forward(self, x, mode:str):\n",
        "    if mode == \"norm\":\n",
        "      self._get_statistics(x)\n",
        "      x = self._normalize(x)\n",
        "    elif mode == \"denorm\":\n",
        "      x = self._denormalize(x)\n",
        "    else: raise NotImplementedError\n",
        "    return x\n",
        "\n",
        "  def _init_params(self):\n",
        "    # initialize InstanceNorm params: (C,)\n",
        "    self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
        "    self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
        "\n",
        "  def _get_statistics(self, x):\n",
        "    dim2reduce = tuple(range(1, x.ndim-1))\n",
        "    if self.subtract_last:\n",
        "      self.last = x[:,-1,:].unsqueeze(1)\n",
        "    else:\n",
        "      self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
        "      self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
        "\n",
        "  def _normalize(self, x):\n",
        "    if self.subtract_last:\n",
        "      x = x - self.last\n",
        "    else:\n",
        "      x = x - self.mean\n",
        "      x = x / self.stdev\n",
        "    if self.affine:\n",
        "      x = x * self.affine_weight\n",
        "      x = x + self.affine_bias\n",
        "    return x\n",
        "\n",
        "  def _denormalize(self, x):\n",
        "    if self.affine:\n",
        "      x = x - self.affine_bias\n",
        "      x = x / (self.affine_weight + self.eps*self.eps)\n",
        "      x = x * self.stdev\n",
        "    if self.subtract_last:\n",
        "      x = x + self.last\n",
        "    else:\n",
        "      x = x + self.mean\n",
        "    return x"
      ],
      "metadata": {
        "id": "TsvSnKiQNUpN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_data = torch.randn(16,7,96)\n",
        "instnorm = InstanceNorm(num_features=7)\n",
        "instnorm(random_data,\"norm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "wGU0UJR4TttY",
        "outputId": "d5b107d7-920c-4221-fc23-fe45186c66c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-fef2e14e0896>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minstnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInstanceNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minstnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"norm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-4a2075d36b87>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mode)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"norm\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"denorm\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_denormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-4a2075d36b87>\u001b[0m in \u001b[0;36m_normalize\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (96) must match the size of tensor b (7) at non-singleton dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup path to data folder and file name\n",
        "data_path = Path(\"data/\")\n",
        "file_name = \"national_illness.csv\"\n",
        "\n",
        "# Sequence length (look-back window)\n",
        "seq_len = 104\n",
        "\n",
        "# Prediciton length (forecast horizon)\n",
        "pred_len = 24\n",
        "\n",
        "# Batch size\n",
        "batch_size = 16\n",
        "\n",
        "# Features of model: M  -> multivariate predict multivariate\n",
        "#                    S  -> univariate predict univariate\n",
        "#                    MS -> multivariate predict univariate\n",
        "features = \"M\"\n",
        "\n",
        "train_dataloader, test_dataloader, vali_dataloader = create_dataloaders(\n",
        "    data_path=data_path,\n",
        "    file_name=file_name,\n",
        "    seq_len=seq_len,\n",
        "    batch_size=batch_size,\n",
        "    pred_len=pred_len,\n",
        "    features=features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPnqG489Jfm3",
        "outputId": "61170805-4af9-4197-b800-0555208cb838"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6IzcVApKbnZ",
        "outputId": "29238bee-53ec-478f-e00b-87ee9c7bcd42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f7e42603220>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    }
  ]
}