{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOT3l+9ojAiksRLljUdBkIM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khauzenberger/pytorch-projects/blob/main/1_long_term_forecasting_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The project\n",
        "\n",
        "In a frist step, I'm going to be replicating the paper \"A Time Series is Worth 64 Words: Long-term Forecasting with Transformers\" by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong and Jayant Kalagnanam (see https://arxiv.org/abs/2211.14730).\n",
        "\n",
        "In the second step, then, I apply the key designs of the paper to forecast macroeconomic time series."
      ],
      "metadata": {
        "id": "9nlO-N-HO_nL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key designs, model overview, and key concept\n",
        "\n",
        "**Patching**: Segmentation of time series into subseries-level patches which are served as input tokens to Transformer.\n",
        "\n",
        "**Channel-independence**: Each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series.\n",
        "\n",
        "The figure below sketches the so-called **PatchTST model** and transformer backbones under supervised and self-supervised learning.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/yuqinie98/PatchTST/main/pic/model.png)\n",
        "\n",
        "**Transformer-based models**: It is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data.\n",
        "\n",
        "Transformers are designed to process sequential input data, processing them all at once. The attention mechanism provides context for any position in the input sequence. \n",
        "\n",
        "Because transformers process the entire input all at once, they allow for more parallelization than recurrent neural networks (RNNs) and therefore reduce training times."
      ],
      "metadata": {
        "id": "4yl7wFdbnSbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replication\n",
        "\n",
        "In the following I provide step-by-step instructions to get from our inputs to the desired outputs.\n",
        "\n",
        "While the paper comes with an official implementation (https://github.com/yuqinie98/PatchTST), there's no learning in just copying and pasting them. That's why I will start more or less from scratch, going through the steps in the section on paper replication in Daniel Bourke's ZTM course \"PyTorch for deep learning\" (https://github.com/khauzenberger/pytorch-deep-learning). "
      ],
      "metadata": {
        "id": "6-7Vu2g5pNrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data from the paper\n",
        "\n",
        "While the paper experiments with eight different datasets, I will focus only on the smallest one: the influenza-like illnesses (ILI) dataset. "
      ],
      "metadata": {
        "id": "VUoAGik7E4y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it... \n",
        "if data_path.is_dir():\n",
        "    print(f\"{data_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {data_path} directory, creating one...\")\n",
        "    data_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Download influenza-like illness data from my Github repo\n",
        "    with open(data_path / \"national_illness.csv\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/khauzenberger/pytorch-projects/raw/main/data/national_illness.csv\")      \n",
        "        print(\"Downloading influenza-like illness data from my Github repo...\")\n",
        "        f.write(request.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_9Ic_aqJOkn",
        "outputId": "c8fc77a2-69f2-479e-d800-11fe4e0fe401"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find data directory, creating one...\n",
            "Downloading influenza-like illness data from my Github repo...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Datasets and DataLoaders"
      ],
      "metadata": {
        "id": "ZxyQudfCvdCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def create_dataloaders(data_path:str, file_name:str, seq_len:int, pred_len:int, \n",
        "                       batch_size:int=1, scale=False,features:str=\"S\",\n",
        "                       target:str=\"OT\"):\n",
        "  \n",
        "  scaler = StandardScaler()\n",
        "  df_raw = pd.read_csv(os.path.join(data_path,file_name))\n",
        "  \n",
        "  cols = list(df_raw.columns)\n",
        "  cols.remove(target)\n",
        "  cols.remove(\"date\")\n",
        "  df_raw = df_raw[[\"date\"] + cols + [target]]\n",
        "\n",
        "  num_train = int(len(df_raw) * 0.7)\n",
        "  num_test = int(len(df_raw) * 0.2)\n",
        "  num_vali = len(df_raw) - num_train - num_test\n",
        "\n",
        "  border1s = [0, num_train - seq_len, len(df_raw) - num_vali - seq_len]\n",
        "  border2s = [num_train, num_train + num_test, len(df_raw)]\n",
        "\n",
        "  if features == \"M\" or features == \"MS\":\n",
        "    cols_data = df_raw.columns[1:]\n",
        "    df_data = df_raw[cols_data]\n",
        "  elif features == \"S\":\n",
        "    df_data = df_raw[[target]]\n",
        "\n",
        "  if scale:\n",
        "    train_data = df_data[border1s[0]:border2s[0]]\n",
        "    scaler.fit(train_data.values)\n",
        "    data = scaler.transform(df_data.values)\n",
        "  else:\n",
        "    data = df_data.values\n",
        "\n",
        "  train_data = data[border1s[0]:border2s[0]]\n",
        "  test_data = data[border1s[1]:border2s[1]]\n",
        "  vali_data = data[border1s[2]:border2s[2]]\n",
        "\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True)\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False)\n",
        "  vali_dataloader = DataLoader(\n",
        "      vali_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False)\n",
        "\n",
        "  return train_dataloader, test_dataloader, vali_dataloader"
      ],
      "metadata": {
        "id": "NZxkLb0MYXMh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model settings"
      ],
      "metadata": {
        "id": "bRnZHGkv0OjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup path to data folder and file name\n",
        "data_path = Path(\"data/\")\n",
        "file_name = \"national_illness.csv\"\n",
        "\n",
        "# Sequence length (look-back window)\n",
        "seq_len = 104\n",
        "\n",
        "# Prediciton length (forecast horizon)\n",
        "pred_len = 24\n",
        "\n",
        "# Batch size\n",
        "batch_size = 16\n",
        "\n",
        "# Features of model: M  -> multivariate predict multivariate\n",
        "#                    S  -> univariate predict univariate\n",
        "#                    MS -> multivariate predict univariate\n",
        "features = \"M\"\n",
        "\n",
        "# Various parameters and hyperparameters\n",
        "d_model = 16\n",
        "patch_len = 24\n",
        "stride = 2\n",
        "n_heads = 4\n",
        "enc_in = 7\n",
        "e_layers = 3\n",
        "d_ff = 128\n",
        "dropout = 0.3\n",
        "fc_dropout = 0.3\n",
        "head_dropout = 0\n",
        "learning_rate = 0.0025\n",
        "padding_patch=\"end\""
      ],
      "metadata": {
        "id": "dPnqG489Jfm3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PatchTST architecture\n",
        "\n",
        "### Instance normalization\n",
        "\n",
        "This technique helps mitigating the distribution shift effect between the training and testing data."
      ],
      "metadata": {
        "id": "BBEgrUA_M9HA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class InstanceNorm(nn.Module):\n",
        "  def __init__(self, num_features:int, eps=1e-5,  affine=True, \n",
        "               subtract_last=False):    \n",
        "    \"\"\"\n",
        "    :param num_features: the number of features or channels\n",
        "    :param eps: a value added for numerical stability\n",
        "    :param affine: if True, InstanceNorm has learnable affine parameters\n",
        "    \"\"\"    \n",
        "    super(InstanceNorm, self).__init__()\n",
        "    self.num_features = num_features\n",
        "    self.eps = eps\n",
        "    self.affine = affine\n",
        "    self.subtract_last = subtract_last\n",
        "    if self.affine:\n",
        "      self._init_params()\n",
        "\n",
        "  def forward(self, x, mode:str):\n",
        "    if mode == \"norm\":\n",
        "      self._get_statistics(x)\n",
        "      x = self._normalize(x)\n",
        "    elif mode == \"denorm\":\n",
        "      x = self._denormalize(x)\n",
        "    else: raise NotImplementedError\n",
        "    return x\n",
        "\n",
        "  def _init_params(self):\n",
        "    # initialize InstanceNorm params: (C,)\n",
        "    self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
        "    self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
        "\n",
        "  def _get_statistics(self, x):\n",
        "    dim2reduce = tuple(range(1, x.ndim-1))\n",
        "    if self.subtract_last:\n",
        "      self.last = x[:,-1,:].unsqueeze(1)\n",
        "    else:\n",
        "      self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
        "      self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
        "\n",
        "  def _normalize(self, x):\n",
        "    if self.subtract_last:\n",
        "      x = x - self.last\n",
        "    else:\n",
        "      x = x - self.mean\n",
        "      x = x / self.stdev\n",
        "    if self.affine:\n",
        "      x = x * self.affine_weight\n",
        "      x = x + self.affine_bias\n",
        "    return x\n",
        "\n",
        "  def _denormalize(self, x):\n",
        "    if self.affine:\n",
        "      x = x - self.affine_bias\n",
        "      x = x / (self.affine_weight + self.eps*self.eps)\n",
        "      x = x * self.stdev\n",
        "    if self.subtract_last:\n",
        "      x = x + self.last\n",
        "    else:\n",
        "      x = x + self.mean\n",
        "    return x"
      ],
      "metadata": {
        "id": "TsvSnKiQNUpN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_data = torch.randn(10,7)\n",
        "instnorm = InstanceNorm(num_features=7)\n",
        "instnorm(random_data,\"norm\")"
      ],
      "metadata": {
        "id": "wGU0UJR4TttY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Patching and Padding\n",
        "\n",
        "Pads the input tensor using replication of the input boundary.\n",
        "\n"
      ],
      "metadata": {
        "id": "AW4NJxzOzvEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.arraypad import pad\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "patch_num = int((seq_len-patch_len)/stride+1)\n",
        "if padding_patch == \"end\":\n",
        "  padding_patch_layer = nn.ReplicationPad1d((0, stride))\n",
        "  patch_num += 1"
      ],
      "metadata": {
        "id": "MU_-nBMAzzzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input encoding (projection)\n",
        "\n",
        "Implemeting equation 1: Projection of feature vectors onto a d-dim vector space."
      ],
      "metadata": {
        "id": "ZfJXKRke1-oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_P = nn.Linear(patch_len, d_model)"
      ],
      "metadata": {
        "id": "Ytk6rxrX2ETs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional encoding (position embedding)"
      ],
      "metadata": {
        "id": "w_SnLQ523NOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from typing import Callable, Optional\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def PositionalEncoding(pe, learn_pe, q_len, d_model):\n",
        "    if pe == None:\n",
        "      W_pos = torch.empty((q_len, d_model))\n",
        "      nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "      learn_pe = False\n",
        "    elif pe == \"zero\":\n",
        "        W_pos = torch.empty((q_len, 1))\n",
        "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "    elif pe == \"zeros\":\n",
        "        W_pos = torch.empty((q_len, d_model))\n",
        "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
        "    elif pe == \"normal\" or pe == \"gauss\":\n",
        "        W_pos = torch.zeros((q_len, 1))\n",
        "        torch.nn.init.normal_(W_pos, mean=0.0, std=0.1)\n",
        "    elif pe == \"uniform\":\n",
        "        W_pos = torch.zeros((q_len, 1))\n",
        "        nn.init.uniform_(W_pos, a=0.0, b=0.1)\n",
        "    else: raise ValueError(f\"{pe} is not a valid pe (positional encoder). Available types: 'gauss'=='normal', \\\n",
        "        'zeros', 'zero', 'uniform'.\")\n",
        "    return nn.Parameter(W_pos, requires_grad=learn_pe)"
      ],
      "metadata": {
        "id": "a1QsS33J3r-h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backbone"
      ],
      "metadata": {
        "id": "dRxRzoVH5tuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, Optional\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class TransformerBackbone(nn.Module):\n",
        "  def __init__(self, c_in:int, context_window:int, target_window:int, patch_len:int,\n",
        "               stride:int, max_seq_len:Optional[int]=1024, n_layers:int=3, d_model=128,\n",
        "               n_heads=16, d_k:Optional[int]=None, d_v:Optional[int]=None, d_ff:int=256, \n",
        "               norm:str='BatchNorm', attn_dropout:float=0., dropout:float=0., act:str=\"gelu\", \n",
        "               key_padding_mask:bool='auto', padding_var:Optional[int]=None, \n",
        "               attn_mask:Optional[Tensor]=None, res_attention:bool=True, pre_norm:bool=False, \n",
        "               store_attn:bool=False, pe:str='zeros', learn_pe:bool=True, fc_dropout:float=0., \n",
        "               head_dropout = 0, padding_patch = None, pretrain_head:bool=False, \n",
        "               head_type = 'flatten', individual = False, normin = True, affine = True, \n",
        "               subtract_last = False, verbose:bool=False, **kwargs):\n",
        "        \n",
        "    super().__init__()\n",
        "\n",
        "    # Instance normalization\n",
        "    self.normin = normin\n",
        "    if normin:\n",
        "      self.normin_layer = InstanceNorm(c_in, eps=1e-5,  affine=affine, \n",
        "                                       subtract_last=subtract_last):\n",
        "    \n",
        "    # Patching and padding\n",
        "    self.patch_len = patch_len\n",
        "    self.stride = stride\n",
        "    self.padding_patch = padding_patch\n",
        "    patch_num = int((seq_len-patch_len)/stride+1)\n",
        "    if padding_patch == \"end\":\n",
        "      self.padding_patch_layer = nn.ReplicationPad1d((0, stride))\n",
        "      patch_num += 1\n",
        "\n",
        "    # Encoder\n",
        "    self.encoder = 1\n",
        "\n",
        "    # Head\n",
        "    self.head_nf = d_model * patch_num\n",
        "    self.n_vars = c_in\n",
        "    self.pretrain_head = pretrain_head\n",
        "    self.head_type = head_type\n",
        "    self.individual = individual    \n",
        "\n",
        "    if self.pretrain_head: \n",
        "     # custom head passed as a partial func with all its kwargs\n",
        "     self.head = self.create_pretrain_head(self.head_nf, c_in, fc_dropout) \n",
        "    elif head_type == 'flatten': \n",
        "      self.head = FlattenHead(self.individual, self.n_vars, self.head_nf, \n",
        "                              target_window, head_dropout=head_dropout)\n",
        "\n",
        "    def create_pretrain_head(self, head_nf, vars, dropout):\n",
        "      return nn.Sequential(nn.Dropout(dropout), nn.Conv1d(head_nf, vars, 1))\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, z):                            \n",
        "      # Instance normalization\n",
        "      if self.normin:\n",
        "        z = z.permute(0,2,1)                                             # z: [bs x nvars x seq_len]\n",
        "        z = self.revin_layer(z, 'norm')\n",
        "        z = z.permute(0,2,1)\n",
        "            \n",
        "      # Patching\n",
        "      if self.padding_patch == 'end':\n",
        "        z = self.padding_patch_layer(z)\n",
        "      z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)  # z: [bs x nvars x patch_num x patch_len]           \n",
        "      z = z.permute(0,1,3,2)                                             # z: [bs x nvars x patch_len x patch_num]                                                             \n",
        "        \n",
        "      # Model\n",
        "      z = self.backbone(z)                                               # z: [bs x nvars x d_model x patch_num]\n",
        "      z = self.head(z)                                                   # z: [bs x nvars x target_window] \n",
        "        \n",
        "      # denorm\n",
        "      if self.revin: \n",
        "        z = z.permute(0,2,1)\n",
        "        z = self.revin_layer(z, 'denorm')\n",
        "        z = z.permute(0,2,1)\n",
        "      return z"
      ],
      "metadata": {
        "id": "BQQVI47x553F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, c_in, patch_num, patch_len, max_seq_len=1024, n_layers=3, \n",
        "               d_model=128, n_heads=16, d_k=None, d_v=None,d_ff=256, norm='BatchNorm', \n",
        "               attn_dropout=0., dropout=0., act=\"gelu\", store_attn=False,\n",
        "               key_padding_mask='auto', padding_var=None, attn_mask=None, \n",
        "               res_attention=True, pre_norm=False, pe='zeros', learn_pe=True, \n",
        "               verbose=False, **kwargs):\n",
        "               \n",
        "    super().__init__()    \n",
        "    \n",
        "    # Input encoding\n",
        "    self.W_P = nn.Linear(patch_len, d_model)\n",
        "\n",
        "    # Positional encoding\n",
        "    self.W_pos = PositionalEncoding(pe, learn_pe, q_len, d_model)\n",
        "\n",
        "    # Residual dropout\n",
        "    self.dropout = nn.Dropout(dropout)"
      ],
      "metadata": {
        "id": "a_OoxV89UgNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FlattenHead(nn.Module):\n",
        "  def __init__(self, individual, n_vars, nf, target_window, head_dropout=0):\n",
        "    super().__init__()\n",
        "        \n",
        "    self.individual = individual\n",
        "    self.n_vars = n_vars\n",
        "        \n",
        "    if self.individual:\n",
        "      self.linears = nn.ModuleList()\n",
        "      self.dropouts = nn.ModuleList()\n",
        "      self.flattens = nn.ModuleList()\n",
        "      for i in range(self.n_vars):\n",
        "        self.flattens.append(nn.Flatten(start_dim=-2))\n",
        "        self.linears.append(nn.Linear(nf, target_window))\n",
        "        self.dropouts.append(nn.Dropout(head_dropout))\n",
        "      else:\n",
        "        self.flatten = nn.Flatten(start_dim=-2)\n",
        "        self.linear = nn.Linear(nf, target_window)\n",
        "        self.dropout = nn.Dropout(head_dropout)\n",
        "            \n",
        "  def forward(self, x):                     # x: [bs x nvars x d_model x patch_num]\n",
        "    if self.individual:\n",
        "      x_out = []\n",
        "      for i in range(self.n_vars):\n",
        "        z = self.flattens[i](x[:,i,:,:])    # z: [bs x d_model * patch_num]\n",
        "        z = self.linears[i](z)              # z: [bs x target_window]\n",
        "        z = self.dropouts[i](z)\n",
        "        x_out.append(z)\n",
        "        x = torch.stack(x_out, dim=1)       # x: [bs x nvars x target_window]\n",
        "      else:\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.dropout(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "TedS_CgeO_ih"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}